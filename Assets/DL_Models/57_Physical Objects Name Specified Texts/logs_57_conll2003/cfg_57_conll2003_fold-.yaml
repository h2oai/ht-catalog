architecture:
    backbone: bert-base-uncased
    dropout: 0
    gradient_checkpointing: false
    intermediate_dropout: 0.0
    pretrained: true
augmentation: {}
dataset:
    data_sample: 1
    data_sample_choice:
    - Train
    - Validation
    folds:
    - '0'
    group_fold_column: id
    label_columns: ner_tags
    test_dataframe: data/anon/conll2003_text_token_classification/data/conll2003/test.pq
    text_column: text
    train_dataframe: data/anon/conll2003_text_token_classification/data/conll2003/train.pq
    validation_dataframe: data/anon/conll2003_text_token_classification/data/conll2003/validation.pq
    validation_size: 0.2
    validation_strategy: custom
environment:
    gpus:
    - '0'
    mixed_precision_inference: false
    mixed_precision_training: true
    number_of_workers: 4
    seed: -1
experiment_name: 57_conll2003
logging:
    ignore_classes: O,NOT_WORD_START
    logger: None
    neptune_project: ''
    number_of_texts: 10
prediction:
    metric: MICRO_F1_SCORE
tokenizer:
    lowercase: false
    max_length: 128
training:
    automatically_adjust_batch_size: false
    batch_size: 16
    build_scoring_pipelines: true
    calculate_train_metric: false
    differential_learning_rate: 1.0e-05
    differential_learning_rate_layers: []
    drop_last_batch: true
    epochs: 2
    evaluation_epochs: 1
    grad_accumulation: 1
    gradient_clip: 0.0
    learning_rate: 1.0e-05
    loss_function: CrossEntropy
    optimizer: AdamW
    save_best_checkpoint: false
    schedule: Cosine
    train_validation_data: false
    warmup_epochs: 0
    weight_decay: 0.0
